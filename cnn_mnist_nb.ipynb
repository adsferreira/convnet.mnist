{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_mnist_nb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJP7quT0c5JFhVejWz3Vhj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsferreira/convnet.mnist/blob/main/cnn_mnist_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDrjN42a_AIA",
        "outputId": "10c36931-f982-4a0f-968e-cc43407f6689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cscO5XGDFw1m"
      },
      "source": [
        "Importando pacotes do Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFdzi0bAF5mn"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaLIRQpGagN"
      },
      "source": [
        "Criando as camadas convolucionais da rede neural artificial, intercaladas com camadas do tipo max pooling 2D:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nVRpuk6G6RV"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PiLqAF2HvwK"
      },
      "source": [
        "Vale notar que uma *convnet* recebe, como entrada, tensores de dimensão *(altura_imagem, largura_imagem, canal)*. Neste caso, configuramos uma *convnet* para processar, como entrada, imagens de dimensãão (28, 28, 1), que é o formato das imagens presentes na base MNIST. Ou seja, estas imagens possuem 28 pixels de altura e largura, e canais com apenas um componente, que é típico de imagens cujos pixels são colorizados em escala de cinza.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ahAIhlPJDIx"
      },
      "source": [
        "Agora, podemos imprimir a arquitetura da *convnet* que configuramos até agora:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR52Jr-sJcIP",
        "outputId": "1d19c6d8-93d9-4a67-e785-af04fc9393da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "585zNPxBJ9K6"
      },
      "source": [
        "Podemos notar, pela saída acima, que a saída de cada camada *Conv2D* e *MaxPooling2D* é um tensor na forma *(altura, largura, canal)*. As alturas e larguras de cada camada tendem a diminuir conforme a rede neural vai ficando mais profunda (adquiri mais camadas). O número de canais é controlado pelo primeiro argumento passado às camadas *Conv2D*: 32 ou 64 nós. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBJytXnLDck"
      },
      "source": [
        "O próximo passo é incluir um classificador densamente conectado na saída da parte convolucional da rede neural. Isto é feito criando uma pilha de camadas do tipo *Dense*. Este tipo de camada processa vetores 1D, enquanto a saída da última camada convolucional é um tensor 3D de dimensão (3, 3, 64), como mostra o sumário da arquitetura acima. Desta forma, primeiramente linearizamos o tensor 3D, de forma que se torne um vetor 1D. Para isso, incluímos uma camada *flatten* utilizando o méétodo *Flatten()* do objeto *layers*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00VrK0s0MdWv"
      },
      "source": [
        "model.add(layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G474YvWAMmJY"
      },
      "source": [
        "Agora, incluimos algumas camadas do tipo *Dense* no topo da rede neural:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNd06AmgMxi-"
      },
      "source": [
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5zc5SXwNXgc"
      },
      "source": [
        "Notem que a última camada possui 10 nós. Esta configuração é necessária, pois existem 10 diferentes classes na base MNIST. Desta forma, um determinado nó na saída deverá ser *mais ativado* quando a rede processar uma imagem que corresponde a sua respectiva classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mq7YmjvN3l2"
      },
      "source": [
        "Conferindo, novamente, se a *convnet* está projetada assim como determinamos no código acima:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX6T3FZuODvX",
        "outputId": "1c5ae456-3e69-4517-823d-21828def6e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJrRXQy2OQaT"
      },
      "source": [
        "Como podemos notar, a saída da última camada convolucional, cuja dimensões são (3, 3, 64), foram linearizadas para um vetor 1D de 576 elementos (3 x 3 x 64 = 576?):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UeASLyROlpK",
        "outputId": "6605452f-7725-4999-f979-3da9da1d7eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(3 * 3 * 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcgZtYtOOuKD"
      },
      "source": [
        "Depois de configurar o modelo, vamos treiná-lo na base do MINST. Primeiramente, carregamos as imagens, que já estão disponíveis no *keras*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH81zeT-PDx3",
        "outputId": "7cc6378c-1889-4543-d606-dd48a55f228b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfQ1zW7JOrWJ"
      },
      "source": [
        "Vamos conhecer as dimensões das imagens de entrada e rótulos presente na base MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEzCUD0uPDTy",
        "outputId": "90e6bf3d-987b-4d9f-eb83-396f0467e001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balhH9mNQhD_"
      },
      "source": [
        "Podemos notar que o conjunto de dados de treinamento possui 60000 imagens de 28x28 pixels. Devemos recordar, no entanto, que o *keras* espera, como entrada, imagens com dimensões *(altura_imagem, largura_imagem, canais)*. Como as imagens na base MNIST estão organizadas com as dimensões de altura e largura apenas, devemos reorganizá-las para também possuírem a dimensão *canais*. Isso pode ser facilmente feito utilizando a função *reshape* disponível no pacote *numpy*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idc6XinRR8vE"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3qkQcKSXII"
      },
      "source": [
        "Notem, acima, que as imagens de teste também precisam apresentar a dimensão *canais*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItYTp5pvSeOW"
      },
      "source": [
        "Podemos checar também o tipo de dado dos elementos da imagem. Para isso, é suficiente checar o tipo de dado do primeiro elemento da matriz *train_images*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_JTNQKMS1e-",
        "outputId": "1cf3f9e1-02aa-47f3-8085-34e2029af082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(train_images[0][0][0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.uint8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-aSfUwrXEJ-"
      },
      "source": [
        "Podemos notar que os elementos da imagem são inteiros de 8 bits sem sinal. De fato, esperamos que os elementos sejam do tipo inteiro. Imprimimos a última dimensão da matriz, pois é na dimensão *canais* que aparecem os valores que definem as cores em cada pixel da imagem. No caso das imagens da base MNIST, as cores são cinzas, que são valores escalares inteiros (precisam de apenas um canal) que variam de 0 (preto) a 255 (branco). No caso de imagens coloridas, é típico utilizar-se a representação RGB (Red-Green-Blue), que necessitam de três canais para representação (um para cada cor, vermelha, verde e azul), e cada canal poderá assumir uma intensidade que varia de 0 a 255.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlWgnmE0dAdf"
      },
      "source": [
        "Podemos checar o menor e o maior valor presente em todas imagens da base MNIST. Para isso, utilizaremos as funções *min* e *max* do *numpy*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJRaYwSdwWu",
        "outputId": "33fceeba-4477-479c-9df2-4fb9b1b9d25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"valor mínimo encontrado nas imagens da base de treinamento: \", train_images.min())\n",
        "print(\"valor máximo encontrado nas imagens da base de treinamento: \", train_images.max())\n",
        "print(\"valor mínimo encontrado nas imagens da base de teste: \", test_images.max())\n",
        "print(\"valor máximo encontrado nas imagens da base de teste: \", test_images.max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valor mínimo encontrado nas imagens da base de treinamento:  0\n",
            "valor máximo encontrado nas imagens da base de treinamento:  255\n",
            "valor mínimo encontrado nas imagens da base de teste:  255\n",
            "valor máximo encontrado nas imagens da base de teste:  255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYF_0OozXEHj"
      },
      "source": [
        "Podemos afirmar que ao menos uma imagem possui as cores preto e branco na base MNIST, pois podemos afirmar que a base possui o valor 0 e 255 para alguns pixels das imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZWi9RdLeyJw"
      },
      "source": [
        "No caso de Redes Neurais Artificiais, é comum normalizarmos os dados de entrada antes do processamento pela rede. Por exemplo, números grandes, como o 255 para representar uma cor, podem ser inadequados para minimização da função custo, pois podem ir para os pontos de saturação das funções de ativação ou gerar gradientes grandes que podem não se adaptar as regiões de minimização. Assim, um tipo de normalização simples é escalar os valores de entrada para o intervalo 0-1, conforme abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnSlTu-xgVNR"
      },
      "source": [
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mshltwENgidX"
      },
      "source": [
        "Notem que os tipos agora devem ser valores reais. Assim, os convertemos para o tipo *float32*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDFDIbJBgrgj"
      },
      "source": [
        "Não diferente, precisamos também adaptar os rótulos. Primeiramente, vamos checar as dimensões dos rótulos presentes na base MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hikN4SmVg_SS",
        "outputId": "503b562b-5e64-4bc6-cec0-d01cf82e26b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZTWXEZfhp7E"
      },
      "source": [
        "Podemos notar que ambos são vetores (de uma dimensão), e que cada vetor possui uma quantidade de rótulos correspondente com o número de imagens em cada conjunto de dados respectivo. De fato, os rótulos são os números inteiros representados por cada imagem, ou seja, os decimais de 0 a 9. Podemos utilizar as funções *min* e *max* para validarmos os valores mínimo e máximo presentes nos vetores acima: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TONbvsBDirUU",
        "outputId": "5bedcdd9-6438-4ae5-ef2e-4811109ee82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"menor valor encontrado nos rótulos de treinamento\", train_labels.min())\n",
        "print(\"maior valor encontrado nos rótulos de treinamento\", train_labels.max())\n",
        "print(\"menor valor encontrado nos rótulos de teste\", test_labels.min())\n",
        "print(\"maior valor encontrado nos rótulos de teste\", test_labels.max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "menor valor encontrado nos rótulos de treinamento 0\n",
            "maior valor encontrado nos rótulos de treinamento 9\n",
            "menor valor encontrado nos rótulos de teste 0\n",
            "maior valor encontrado nos rótulos de teste 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snvoc7aZjN-Z"
      },
      "source": [
        "No caso de classificadores (como o que estamos desenvolvendo aqui), a camada de saída tipicamente possuí um número de neurônios de saída igual ao número de classes presentes no problema. Como definido na nossa arquitetura, colocamos 10 neurônios na última camada, pois temos um total de 10 classes (decimais de 0 a 9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uy1Xwtuj3qj"
      },
      "source": [
        "Há um motivo simples para arquitetar a saída da rede desta maneira: queremos modelar a rede de forma que quando esta receba uma imagem com o caracter zero desenhado, seu primeiro neurônio de saída ative mais que o restante dos neurônios de saída, ou seja, o neurônio que ativou responda com saída 1 e o restante com saída 0, e assim por diante. Assim, vamos mudar a representação dos rótulos: ao invés de utilizar apenas o número presente no rótulo, faremos com que esse número seja representado por um vetor de 10 posições (pois temos 10 classes), e que este vetor contenha todos elementos com valores zero, exceto o elemento que está na posição representada pelo respectivo número. Este elemento recebe o valor 1. Por exemplo, um rótulo com valor 9 será representado por um vetor de zeros em todas posições, exceto na posição 9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45SrX-kEno2V"
      },
      "source": [
        "De fato, a intenção é modelarmos a rede para que seu último neurônio de saída dê como resposta o valor 1, e todo o restante responda com zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kNuX7Pun94q"
      },
      "source": [
        "Como este tipo de mudança é bem típico, o *keras* já provê esta funcionalidade através da função *to_categorical*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMNb9JYWoLQz"
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTY2lqnVosjI"
      },
      "source": [
        "Com a arquitetura da rede e base de dados configuradas, resta-nos apenas configurar como se dará o treinamento e executá-lo. Os elementos básicos que configuraremos são o otimizador que realizará a modelagem (algoritmo de treinamento em si), o tipo de função custo (*loss function*) a analisar, e a métrica de desempenho usada como referência. O otimizador usado será o *rmsprop*, a função custo será a *categorical_crossentropy* (tipicamente usada para classificadores multi-classes), e a métrica será a acurácia de resposta do modelo. Esta configuração é realizada pela função *compile* do objeto *model*: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoH2A6OVqLT5"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR_dV0bHqRJJ"
      },
      "source": [
        "Agora podemos realizar o treinamento para obtenção do modelo de predição de caracteres. Para isso, o *keras* ajustará o modelo para criar a relação entre imagens e os respectivos rótulos de treinamento, através da função *fit* do objeto *model*. Precisamos indicar quantas épocas (iterações) a rede será treinada, assim como o número de amostras usadas para validação do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNkcvl2LrJ8_",
        "outputId": "dbe5b420-caba-4e5d-8f09-e1dc12a9e4f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.1815 - accuracy: 0.9427\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0482 - accuracy: 0.9848\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0327 - accuracy: 0.9894\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0248 - accuracy: 0.9923\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0193 - accuracy: 0.9941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe69d72afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSkrKn8smqs"
      },
      "source": [
        "Na saída gerada durante o treinamento, acima, temos os valores tanto da função custo quanto da acurácia ao longo das épocas. É possível notar que ambos valores caem ao longo do treinamento, de forma que a rede melhore seu desempenho ao longo do treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdU9-yERtGyH"
      },
      "source": [
        "Mas a validação a respeito da qualidade do modelo só ocorre ao analisarmos seu desempenho sobre o conjunto de teste, pois estes dados são desconhecidos para a rede (nunca foram apresentado para teste modelo durante o treinamento). Abaixo, checamos o desempenho com a função *evaluate* do objeto *model*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQEq23EjtjHT",
        "outputId": "631a7b5c-bcc7-422b-cc3e-670fb46569c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(\"valor da função custo no conjunto de testes: \", test_loss)\n",
        "print(\"acurácia do modelo no conjunto de testes: \", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9921\n",
            "valor da função custo no conjunto de testes:  0.02624332904815674\n",
            "acurácia do modelo no conjunto de testes:  0.9921000003814697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWar2ETgt2-z"
      },
      "source": [
        "Como podemos notar, nossa rede tem acurácia de aproximadamente 0.99 no conjunto de teste, o que significa que o modelo, ao processar 10000 imagens de testes, classificou corretamente aproximadamente 99% das imagens."
      ]
    }
  ]
}